import requests
import pandas as pd
import time
import random
from datetime import datetime
import os
import json

# ========= é…ç½®åŒº =========

# ä»£ç†ï¼ˆå¦‚æœä¸ç”¨ä»£ç†ï¼Œå°±æ”¹æˆ PROXIES = Noneï¼‰
PROXIES = {
    "http": "http://127.0.0.1:7890",
    "https": "http://127.0.0.1:7890",
}

HEADERS = {
    "User-Agent": (
        "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
        "AppleWebKit/537.36 (KHTML, like Gecko) "
        "Chrome/123.0.0.0 Safari/537.36"
    ),
    "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,"
              "image/avif,image/webp,*/*;q=0.8"
}

# âœ… è¿™é‡Œå·²ç»å¸®ä½ æ”¹æˆå½“å‰å¸–å­ CSV çš„åå­—
# æ³¨æ„ï¼šæˆ‘ä»¬ä¼šè®©ä½ åœ¨ â€œZDM+Redditâ€ æ ¹ç›®å½•ä¸‹è¿è¡Œè„šæœ¬ï¼Œè¿™æ ·è·¯å¾„å°±æ˜¯å¯¹çš„
POSTS_CSV = "data_reddit_20251206_103022.csv"

# è¯„è®ºè¾“å‡ºçš„æ–°æ–‡ä»¶ï¼ˆåœ¨æ ¹ç›®å½•ä¸‹ç”Ÿæˆï¼‰
COMMENTS_CSV = f"data_reddit_comments_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv"

# è¯„è®ºæŠ“å–è¿›åº¦æ–‡ä»¶ï¼ˆä¹Ÿåœ¨æ ¹ç›®å½•ï¼‰
COMMENTS_PROGRESS = "reddit_comments_progress.json"


# ========= å·¥å…·å‡½æ•° =========

def append_comment_row(row: dict):
    """æŠŠä¸€æ¡è¯„è®ºå†™å…¥åˆ°è¯„è®º CSVï¼ˆå®æ—¶è¿½åŠ ï¼‰"""
    df = pd.DataFrame([row])
    file_exists = os.path.exists(COMMENTS_CSV)
    df.to_csv(
        COMMENTS_CSV,
        mode="a",
        index=False,
        encoding="utf-8-sig",
        header=not file_exists,
    )


def load_progress(num_posts: int) -> dict:
    """è¯»å–è¿›åº¦ï¼ˆå¤„ç†åˆ°ç¬¬å‡ ä¸ªå¸–å­äº†ï¼‰"""
    if not os.path.exists(COMMENTS_PROGRESS):
        return {"posts_csv": POSTS_CSV, "last_index": -1}

    try:
        with open(COMMENTS_PROGRESS, "r", encoding="utf-8") as f:
            p = json.load(f)
    except Exception:
        return {"posts_csv": POSTS_CSV, "last_index": -1}

    # å¦‚æœå¸–å­ CSV æ¢äº†ï¼Œå°±ä»å¤´å¼€å§‹
    if p.get("posts_csv") != POSTS_CSV:
        return {"posts_csv": POSTS_CSV, "last_index": -1}

    last = p.get("last_index", -1)
    if not isinstance(last, int) or last >= num_posts:
        last = -1

    return {"posts_csv": POSTS_CSV, "last_index": last}


def save_progress(progress: dict):
    """ä¿å­˜è¿›åº¦"""
    with open(COMMENTS_PROGRESS, "w", encoding="utf-8") as f:
        json.dump(progress, f, ensure_ascii=False, indent=2)


def load_seen_comment_ids() -> set:
    """ä»å†å²è¯„è®º CSV ä¸­è¯»å‡ºå·²ç»ä¿å­˜è¿‡çš„è¯„è®º IDï¼Œé¿å…é‡å¤"""
    seen = set()
    if os.path.exists(COMMENTS_CSV):
        try:
            df = pd.read_csv(COMMENTS_CSV, usecols=["source_id"])
            seen = set(df["source_id"].dropna().astype(str).tolist())
        except Exception as e:
            print(f"âš ï¸ è¯»å–å·²æœ‰è¯„è®º CSV å¤±è´¥ï¼ˆå¿½ç•¥ï¼Œå¯èƒ½æ˜¯ç¬¬ä¸€æ¬¡è·‘ï¼‰: {e}")
    return seen


def fetch_comments_for_post(
    post_id: str,
    post_permalink: str,
    brand_id: str,
    phone_model_id: str,
    post_source_id: str,
    seen_comment_ids: set,
    max_comments: int = 50,
) -> int:
    """æŠ“å–æŸä¸€ä¸ªå¸–å­ä¸‹é¢çš„è¯„è®ºï¼Œè¿”å›æœ¬æ¬¡æ–°æŠ“åˆ°çš„è¯„è®ºæ•°é‡"""

    url = f"https://www.reddit.com/comments/{post_id}.json"

    try:
        resp = requests.get(
            url,
            headers=HEADERS,
            timeout=20,
            proxies=PROXIES,
            params={"limit": max_comments, "sort": "top"},
        )
    except Exception as e:
        print(f"      âš ï¸ è¯·æ±‚è¯„è®ºå¤±è´¥: {e}")
        return 0

    if resp.status_code != 200:
        print(f"      âš ï¸ è¯·æ±‚è¯„è®ºå¤±è´¥ status={resp.status_code}")
        return 0

    try:
        data = resp.json()
    except Exception as e:
        print(f"      âš ï¸ è§£æè¯„è®º JSON å¤±è´¥: {e}")
        return 0

    # comments æ¥å£æ˜¯ä¸€ä¸ª listï¼Œç¬¬ 2 ä¸ªå…ƒç´ æ‰æ˜¯è¯„è®ºæ ‘
    if not isinstance(data, list) or len(data) < 2:
        return 0

    comments_listing = data[1].get("data", {}).get("children", [])
    count_new = 0

    for item in comments_listing:
        # kind == t1 æ‰æ˜¯è¯„è®ºæœ¬ä½“ï¼›t3 æ˜¯å¸–å­ï¼›more æ˜¯â€œæ›´å¤šè¯„è®ºâ€
        if item.get("kind") != "t1":
            continue

        c = item.get("data", {})
        body = (c.get("body") or "").strip()
        if not body:
            continue

        cid = c.get("id")
        if not cid:
            continue

        comment_source_id = f"reddit_comment_{cid}"
        if comment_source_id in seen_comment_ids:
            continue
        seen_comment_ids.add(comment_source_id)

        created_ts = c.get("created_utc")
        if created_ts:
            published_str = datetime.fromtimestamp(created_ts).strftime(
                "%Y-%m-%d %H:%M:%S"
            )
        else:
            published_str = ""

        comment_permalink = c.get("permalink")
        if comment_permalink:
            comment_url = "https://www.reddit.com" + comment_permalink
        else:
            comment_url = "https://www.reddit.com" + (post_permalink or "")

        # å¤šäº† parent_source_idï¼Œæ–¹ä¾¿è·Ÿå¸–å­å¯¹åº”èµ·æ¥
        row = {
            "platform": "reddit",
            "source_id": comment_source_id,
            "source_type": "comment",
            "parent_source_id": post_source_id,
            "url": comment_url,
            "brand_id": brand_id,
            "phone_model_id": phone_model_id,
            "lang": "en",
            "published_at": published_str,
            "raw_text": body[:500],
            "cleaned_text": body[:500],
        }

        append_comment_row(row)
        count_new += 1

        if count_new >= max_comments:
            break

    return count_new


# ========= ä¸»é€»è¾‘ =========

def main():
    # 1. è¯»å¸–å­ CSV
    df_posts = pd.read_csv(POSTS_CSV)
    print(f"ğŸ“„ ä»å¸–å­ CSV è¯»å–åˆ° {len(df_posts)} è¡Œæ•°æ®ï¼ˆæ¯è¡Œä¸€ä¸ªå¸–å­ï¼‰")

    # 2. å·²æœ‰è¯„è®º IDï¼Œé˜²æ­¢é‡å¤
    seen_comment_ids = load_seen_comment_ids()
    print(f"ğŸ” å·²ä»å†å²è¯„è®º CSV è¯»å– {len(seen_comment_ids)} æ¡è¯„è®º IDï¼Œé¿å…é‡å¤")

    # 3. è¿›åº¦ï¼ˆå¤„ç†åˆ°ç¬¬å‡ ä¸ªå¸–å­ï¼‰
    progress = load_progress(len(df_posts))
    last_index = progress["last_index"]
    print(f"â© å°†ä»ç¬¬ {last_index + 1} è¡Œå¸–å­å¼€å§‹æŠ“è¯„è®º")

    total_new_comments = 0

    for idx, row in enumerate(df_posts.itertuples()):
        if idx <= last_index:
            continue

        post_source_id = getattr(row, "source_id")
        # å¸–å­é‡Œæ˜¯ reddit_{id} è¿™ç§å½¢å¼
        post_id = str(post_source_id).replace("reddit_", "", 1)

        brand_id = getattr(row, "brand_id", "Other")
        phone_model_id = getattr(row, "phone_model_id", "")
        post_url = getattr(row, "url", "")
        post_permalink = post_url.replace("https://www.reddit.com", "")

        print(f"\n--- [{idx + 1}/{len(df_posts)}] å¸–å­ {post_source_id} ---")

        n = fetch_comments_for_post(
            post_id=post_id,
            post_permalink=post_permalink,
            brand_id=brand_id,
            phone_model_id=phone_model_id,
            post_source_id=post_source_id,
            seen_comment_ids=seen_comment_ids,
            max_comments=50,   # æ¯ä¸ªå¸–å­æœ€å¤šæŠ“ 50 æ¡è¯„è®ºï¼Œéœ€è¦çš„è¯å¯ä»¥æ”¹
        )

        print(f"      âœ… æ–°æŠ“åˆ°è¯„è®º {n} æ¡")

        total_new_comments += n

        # æ›´æ–°è¿›åº¦
        progress["last_index"] = idx
        save_progress(progress)

        # æ­‡ä¸€ä¼šå„¿ï¼Œé˜²æ­¢ 429
        time.sleep(random.uniform(1, 2.5))

    print(f"\nğŸ‰ å®Œæˆï¼æœ¬æ¬¡å…±æ–°å¢è¯„è®º {total_new_comments} æ¡ï¼Œå·²å†™å…¥ {COMMENTS_CSV}")


if __name__ == "__main__":
    main()
