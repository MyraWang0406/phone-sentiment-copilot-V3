import os
import time
import random
import urllib.parse
from datetime import datetime

import requests
from bs4 import BeautifulSoup
import pandas as pd

import config

# -------- åŸºæœ¬è®¾ç½® --------
CSV_FILENAME = "data_gsmarena_notebookcheck.csv"

MAX_DEVICES_PER_MODEL = 1        # æ¯ä¸ªå…³é”®è¯æœ€å¤šå–å‡ ä¸ªæœç´¢ç»“æœï¼ˆä¸€èˆ¬ 1 ä¸ªå¤Ÿç”¨ï¼‰
MAX_PAGES_PER_DEVICE = 10        # GSMArena opinions æœ€å¤šç¿»å‡ é¡µ
MAX_OPINIONS_PER_DEVICE = 100    # æ¯ä¸ªæœºå‹æœ€å¤šæŠ“å¤šå°‘æ¡ opinion

MAX_COMMENTS_PER_REVIEW = 50     # Notebookcheck æ¯ç¯‡è¯„æµ‹æœ€å¤šæŠ“å¤šå°‘æ¡è¯„è®º

HEADERS = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
                  "AppleWebKit/537.36 (KHTML, like Gecko) "
                  "Chrome/124.0 Safari/537.36"
}

GS_BASE = "https://www.gsmarena.com"
NB_BASE = "https://www.notebookcheck.net"

# Notebookcheck ä½¿ç”¨ Google Custom Searchï¼Œè¿™é‡Œç”¨ä½ æä¾›çš„æ¨¡æ¿
NB_SEARCH_URL_TEMPLATE = (
    "https://www.notebookcheck.net/Google-Search.36690.0.html"
    "?cx=partner-pub-9323363027260837%3Atxif1w-xjer"
    "&cof=FORID%3A10&ie=UTF-8&q={q}&search="
)

# -------- æ–­ç‚¹ç»­è·‘ç”¨çš„é›†åˆ / ç»Ÿè®¡ --------
SEEN_GS_OPINION_KEYS = set()    # (phone_model_id, device_name, raw_text[:80])
GS_OPINION_COUNT = {}           # (phone_model_id, device_name) -> å†å² opinion æ•°

SEEN_NB_ARTICLE_URLS = set()    # Notebookcheck è¯„æµ‹ URL
SEEN_NB_COMMENT_KEYS = set()    # (url, raw_text[:80])


def append_row_to_csv(row: dict):
    """å®æ—¶å°†ä¸€æ¡è®°å½•è¿½åŠ å†™å…¥ CSVï¼Œæ–‡ä»¶ä¸å­˜åœ¨æ—¶è‡ªåŠ¨å†™è¡¨å¤´"""
    df = pd.DataFrame([row])
    file_exists = os.path.exists(CSV_FILENAME)
    df.to_csv(
        CSV_FILENAME,
        mode="a",
        index=False,
        encoding="utf-8-sig",
        header=not file_exists,
    )


def get_soup(url: str, sleep_range=(0.5, 1.2)):
    """å¸¦ headers + ç®€å•é‡è¯•çš„ GETï¼Œç„¶åè¿”å› BeautifulSoup"""
    for i in range(3):
        try:
            resp = requests.get(url, headers=HEADERS, timeout=15)
            if resp.status_code == 200:
                time.sleep(random.uniform(*sleep_range))
                return BeautifulSoup(resp.text, "lxml")
            else:
                print(f"   âš ï¸ è¯·æ±‚ {url} è¿”å› {resp.status_code}")
        except Exception as e:
            print(f"   âš ï¸ è¯·æ±‚ {url} å‡ºé”™: {e}")
        time.sleep(1.0 + i)
    return None


def load_existing_progress():
    """å¯åŠ¨æ—¶è¯»å–æ—§ CSVï¼Œæ„å»ºå»é‡ç”¨çš„é›†åˆï¼Œå®ç°æ–­ç‚¹ç»­è·‘"""
    global GS_OPINION_COUNT

    if not os.path.exists(CSV_FILENAME):
        print(f"ğŸ“ æœªå‘ç°å†å²æ–‡ä»¶ï¼Œå°†ä»å¤´å¼€å§‹æŠ“å–å¹¶å†™å…¥ {CSV_FILENAME}")
        return

    try:
        df = pd.read_csv(CSV_FILENAME)
    except Exception as e:
        print(f"âš ï¸ è¯»å–å†å² CSV å¤±è´¥ï¼Œä¸å¯ç”¨æ–­ç‚¹ç»­çˆ¬é€»è¾‘: {e}")
        return

    if df.empty:
        print(f"ğŸ“‚ å‘ç°å†å²æ–‡ä»¶ {CSV_FILENAME}ï¼Œä½†ä¸ºç©ºï¼Œå°†ä»å¤´å¼€å§‹æŠ“ã€‚")
        return

    print(f"ğŸ“‚ æ£€æµ‹åˆ°å·²æœ‰å†å²æ–‡ä»¶ {CSV_FILENAME}ï¼Œæ€»è®°å½• {len(df)} è¡Œã€‚")

    # GSMArena opinions å»é‡ + è®¡æ•°ï¼ˆè™½ç„¶è¿™æ¬¡ä¸è·‘ GSMArenaï¼Œä½†ä¿ç•™ä¿¡æ¯æ²¡åå¤„ï¼‰
    if {"platform", "data_type", "phone_model_id", "device_name", "raw_text"}.issubset(df.columns):
        gsm_df = df[(df["platform"] == "gsmarena") & (df["data_type"] == "opinion")].copy()
        for _, row in gsm_df.iterrows():
            key = (
                str(row.get("phone_model_id", "")),
                str(row.get("device_name", "")),
                str(row.get("raw_text", ""))[:80],
            )
            SEEN_GS_OPINION_KEYS.add(key)

        if not gsm_df.empty:
            counts = (
                gsm_df.groupby(["phone_model_id", "device_name"])["raw_text"]
                .count()
                .to_dict()
            )
            GS_OPINION_COUNT = {tuple(k): int(v) for k, v in counts.items()}

        print(f"   ğŸ” å·²æœ‰ GSMArena opinions å»é‡é”® {len(SEEN_GS_OPINION_KEYS)} ä¸ª")
        print(f"   ğŸ” å·²æœ‰ GSMArena æœºå‹-è®¾å¤‡ç»„åˆ {len(GS_OPINION_COUNT)} ä¸ª")

    # Notebookcheck è¯„æµ‹æ­£æ–‡ + è¯„è®ºå»é‡
    if {"platform", "data_type", "url"}.issubset(df.columns):
        nb_df = df[df["platform"] == "notebookcheck"].copy()

        art_df = nb_df[nb_df["data_type"] == "review_article"]
        for u in art_df["url"].dropna().unique():
            SEEN_NB_ARTICLE_URLS.add(str(u))
        print(f"   ğŸ” å·²æœ‰ Notebookcheck è¯„æµ‹ {len(SEEN_NB_ARTICLE_URLS)} ç¯‡")

        if "raw_text" in nb_df.columns:
            comm_df = nb_df[nb_df["data_type"] == "review_comment"]
            for _, row in comm_df.iterrows():
                key = (
                    str(row.get("url", "")),
                    str(row.get("raw_text", ""))[:80],
                )
                SEEN_NB_COMMENT_KEYS.add(key)
            print(f"   ğŸ” å·²æœ‰ Notebookcheck è¯„è®ºå»é‡é”® {len(SEEN_NB_COMMENT_KEYS)} ä¸ª")


# =========================
# 1. ï¼ˆä¿ç•™ï¼‰GSMArenaï¼šæœç´¢ + opinions
# =========================

def search_gsmarena_devices(search_kw: str):
    """åœ¨ GSMArena æœç´¢æœºå‹ï¼Œè¿”å›è®¾å¤‡é¡µ URL åˆ—è¡¨"""
    q = urllib.parse.quote_plus(search_kw)
    url = f"{GS_BASE}/results.php3?sQuickSearch=yes&sName={q}"
    print(f"   ğŸ” GSMArena æœç´¢: {url}")
    soup = get_soup(url)
    if not soup:
        return []

    results = []
    makers = soup.select("div.makers ul li a")
    for a in makers:
        href = a.get("href") or ""
        name = (a.get_text() or "").strip()
        if not href:
            continue
        if not href.startswith("http"):
            href = GS_BASE + "/" + href.lstrip("/")
        results.append((name, href))
    print(f"   âœ… GSMArena æœç´¢ç»“æœ {len(results)} ä¸ª")
    return results


def find_gsmarena_opinions_url(device_url: str):
    """ä»è®¾å¤‡é¡µé‡Œæ‰¾åˆ° opinionsï¼ˆç”¨æˆ·è¯„è®ºï¼‰çš„é“¾æ¥"""
    soup = get_soup(device_url)
    if not soup:
        return None

    # 1ï¼‰ä¼˜å…ˆæ‰¾æ–‡æœ¬å¸¦ "opinions" / "opinion" çš„é“¾æ¥
    for a in soup.find_all("a"):
        text = (a.get_text() or "").lower()
        href = a.get("href") or ""
        # åŠ æ‹¬å·é¿å… and / or ä¼˜å…ˆçº§é—®é¢˜
        if ("opinion" in text and "review" in href) or ("-reviews-" in href):
            if not href.startswith("http"):
                href = GS_BASE + "/" + href.lstrip("/")
            return href

    # 2ï¼‰å…œåº•ï¼šæ‰¾ href é‡Œå¸¦ "-reviews-" çš„é“¾æ¥
    for a in soup.find_all("a"):
        href = a.get("href") or ""
        if "-reviews-" in href:
            if not href.startswith("http"):
                href = GS_BASE + "/" + href.lstrip("/")
            return href

    return None


def crawl_gsmarena_opinions(device_name: str, device_url: str,
                            phone_model_id: str, search_kw: str):
    """å¯¹å•ä¸ªè®¾å¤‡ï¼ŒæŠ“ opinions å¤šé¡µï¼ˆä¿ç•™å‡½æ•°ï¼Œä¸åœ¨æœ¬è½®è°ƒç”¨ï¼‰"""
    base_key = (phone_model_id, device_name)
    already = GS_OPINION_COUNT.get(base_key, 0)
    if already >= MAX_OPINIONS_PER_DEVICE:
        print(f"      â­ {device_name} å·²æœ‰æ„è§ {already} æ¡ï¼Œè¾¾åˆ°ä¸Šé™ {MAX_OPINIONS_PER_DEVICE}ï¼Œè·³è¿‡ã€‚")
        return 0

    opinions_total = 0

    opinions_url = find_gsmarena_opinions_url(device_url)
    if not opinions_url:
        print(f"      âš ï¸ {device_name} æ‰¾ä¸åˆ° opinions é“¾æ¥ï¼Œè·³è¿‡ã€‚")
        return opinions_total

    print(f"      ğŸ’¬ opinions èµ·å§‹é¡µ: {opinions_url} (å†å²å·²æœ‰ {already} æ¡)")

    for page_no in range(1, MAX_PAGES_PER_DEVICE + 1):
        if already + opinions_total >= MAX_OPINIONS_PER_DEVICE:
            print(f"      â›” æœ¬è®¾å¤‡ opinions å·²è¾¾åˆ°ä¸Šé™ {MAX_OPINIONS_PER_DEVICE}ï¼Œåœæ­¢ç¿»é¡µã€‚")
            break

        if page_no == 1:
            url = opinions_url
        else:
            if "?" in opinions_url:
                base = opinions_url.split("?", 1)[0]
            else:
                base = opinions_url
            url = f"{base}?page={page_no}"

        print(f"      ğŸ‘‰ GSMArena opinions ç¬¬ {page_no} é¡µ: {url}")
        soup = get_soup(url)
        if not soup:
            print("         âš ï¸ æœ¬é¡µè¯·æ±‚å¤±è´¥ï¼Œåœæ­¢ç¿»é¡µã€‚")
            break

        items = (
            soup.select(".user-thread .uopin")
            or soup.select(".opinions li")
        )
        print(f"         ğŸ” æœ¬é¡µç–‘ä¼¼ opinion å…ƒç´ æ•°: {len(items)}")

        if not items:
            if page_no == 1:
                print("         âš ï¸ æ²¡æ‰¾åˆ°ä»»ä½• opinion å…ƒç´ ï¼Œå¯èƒ½éœ€æ‰‹åŠ¨è°ƒæ•´é€‰æ‹©å™¨ã€‚")
            break

        for ele in items:
            if already + opinions_total >= MAX_OPINIONS_PER_DEVICE:
                break

            text = (ele.get_text() or "").strip()
            if not text:
                continue

            key = (phone_model_id, device_name, text[:80])
            if key in SEEN_GS_OPINION_KEYS:
                continue
            SEEN_GS_OPINION_KEYS.add(key)

            user = ""
            date_str = ""

            parent = ele.parent
            if parent:
                uname = parent.select_one(".uname, .user-nick")
                if uname:
                    user = (uname.get_text() or "").strip()

                dt = parent.select_one(".time, .opinion-date")
                if dt:
                    date_str = (dt.get_text() or "").strip()

            row = {
                "platform": "gsmarena",
                "data_type": "opinion",
                "phone_model_id": phone_model_id,
                "search_kw": search_kw,
                "device_name": device_name,
                "url": url,
                "author": user,
                "time_str": date_str,
                "raw_text": text,
                "cleaned_text": text,
                "created_at": datetime.now().isoformat(timespec="seconds"),
            }
            append_row_to_csv(row)
            opinions_total += 1

        print(f"         âœ… æœ¬é¡µæ–°å¢ {opinions_total} æ¡ï¼Œæœ¬è®¾å¤‡å†å²+æœ¬æ¬¡å…± {already + opinions_total} æ¡ã€‚")

    GS_OPINION_COUNT[base_key] = already + opinions_total
    return opinions_total


# =========================
# 2. Notebookcheckï¼šæŠ“è¯„æµ‹æ­£æ–‡ + è¯„è®º
# =========================

def search_notebookcheck_reviews(search_kw: str, max_results: int = 2):
    """Notebookcheck æœç´¢ï¼Œè¿”å›å¯èƒ½çš„è¯„æµ‹é“¾æ¥åˆ—è¡¨"""
    q = urllib.parse.quote_plus(search_kw)
    url = NB_SEARCH_URL_TEMPLATE.format(q=q)   # â˜… ç”¨ Google-Search æ¨¡æ¿
    print(f"   ğŸ” Notebookcheck æœç´¢: {url}")
    soup = get_soup(url)
    if not soup:
        return []

    links = []
    for a in soup.select("a"):
        href = a.get("href") or ""
        text = (a.get_text() or "").strip()
        if not href or not text:
            continue
        if "review" in href.lower():
            if not href.startswith("http"):
                href = NB_BASE.rstrip("/") + "/" + href.lstrip("/")
            links.append((text, href))

    uniq = []
    seen = set()
    for title, href in links:
        if href in seen:
            continue
        uniq.append((title, href))
        seen.add(href)
        if len(uniq) >= max_results:
            break

    print(f"   âœ… Notebookcheck è¯„æµ‹å€™é€‰ {len(uniq)} æ¡")
    return uniq


def crawl_notebookcheck_review(title: str, url: str,
                               phone_model_id: str, search_kw: str):
    """æŠ“å•ç¯‡ Notebookcheck è¯„æµ‹æ­£æ–‡ + è¯„è®ºï¼ˆæœ€å¤š 50 æ¡è¯„è®ºï¼‰"""
    print(f"      ğŸ“° Notebookcheck è¯„æµ‹: {title[:50]}... -> {url}")
    soup = get_soup(url)
    if not soup:
        print("         âš ï¸ è¯·æ±‚å¤±è´¥ï¼Œè·³è¿‡ã€‚")
        return

    # ------- æ­£æ–‡ -------
    article = (
        soup.select_one("article")
        or soup.select_one("div#content")
        or soup.select_one("div.text")
    )
    text = ""
    if article:
        paras = [(p.get_text() or "").strip() for p in article.select("p")]
        text = "\n".join([p for p in paras if p])

    if text.strip() and url not in SEEN_NB_ARTICLE_URLS:
        row = {
            "platform": "notebookcheck",
            "data_type": "review_article",
            "phone_model_id": phone_model_id,
            "search_kw": search_kw,
            "device_name": title,
            "url": url,
            "author": "",
            "time_str": "",
            "raw_text": text,
            "cleaned_text": text,
            "created_at": datetime.now().isoformat(timespec="seconds"),
        }
        append_row_to_csv(row)
        SEEN_NB_ARTICLE_URLS.add(url)
        print(f"         âœ… Notebookcheck è¯„æµ‹æ­£æ–‡å·²å†™å…¥ CSV ï¼ˆçº¦ {len(text)} å­—ï¼‰")
    elif not text.strip():
        print("         âš ï¸ æ­£æ–‡ä¸ºç©ºï¼Œè·³è¿‡æ­£æ–‡ã€‚")
    else:
        print("         â­ æ­£æ–‡å·²æŠ“è¿‡ï¼Œè·³è¿‡æ­£æ–‡å†™å…¥ã€‚")

    # ------- è¯„è®º -------
    comments_container = (
        soup.select_one("div.comments")
        or soup.select_one("div#comments")
        or soup.select_one("section.comments")
    )
    if not comments_container:
        print("         â„¹ï¸ æœªæ‰¾åˆ°æ˜æ˜¾çš„è¯„è®ºå®¹å™¨ï¼ˆdiv.comments / #comments / section.commentsï¼‰ã€‚")
        return

    comment_items = (
        comments_container.select(".comment")
        or comments_container.find_all("li")
        or comments_container.find_all("div")
    )
    print(f"         ğŸ’¬ ç–‘ä¼¼è¯„è®ºæ¡ç›® {len(comment_items)} ä¸ª")

    written = 0
    for c in comment_items:
        if written >= MAX_COMMENTS_PER_REVIEW:
            break

        content_ele = (
            c.select_one(".comment_text")
            or c.select_one(".text")
            or c.select_one("p")
        )
        content = (content_ele.get_text() or "").strip() if content_ele else ""
        if not content:
            continue

        key = (url, content[:80])
        if key in SEEN_NB_COMMENT_KEYS:
            continue
        SEEN_NB_COMMENT_KEYS.add(key)

        author_ele = (
            c.select_one(".user")
            or c.select_one(".author")
            or c.select_one(".name")
        )
        author = (author_ele.get_text() or "").strip() if author_ele else ""

        time_ele = (
            c.select_one(".date")
            or c.select_one(".time")
        )
        time_str = (time_ele.get_text() or "").strip() if time_ele else ""

        row = {
            "platform": "notebookcheck",
            "data_type": "review_comment",
            "phone_model_id": phone_model_id,
            "search_kw": search_kw,
            "device_name": title,
            "url": url,
            "author": author,
            "time_str": time_str,
            "raw_text": content,
            "cleaned_text": content,
            "created_at": datetime.now().isoformat(timespec="seconds"),
        }
        append_row_to_csv(row)
        written += 1

    print(f"         âœ… Notebookcheck è¯„è®ºæœ¬è½®å†™å…¥ {written} æ¡ï¼ˆå»é‡åï¼‰")


# =========================
# 3. ä»… Notebookcheckï¼šæŒ‰æœºå‹å¾ªç¯
# =========================

def crawl_notebookcheck_only():
    print("ğŸš€ å¼€å§‹ã€ä»…ã€‘æŠ“å– Notebookcheck è¯„æµ‹")
    print(f"ğŸ“ æ•°æ®å†™å…¥: {CSV_FILENAME}")

    load_existing_progress()

    for idx, (model_key, keywords) in enumerate(config.TARGET_MODELS.items(), start=1):
        if not keywords:
            continue

        search_kw = keywords[0]
        print(f"\n================ æœºå‹ {idx}: {model_key} ({search_kw}) ================")

        reviews = search_notebookcheck_reviews(search_kw, max_results=1)
        if not reviews:
            print("   âš ï¸ Notebookcheck æœªæ‰¾åˆ°è¯„æµ‹ã€‚")
        else:
            for title, url in reviews:
                crawl_notebookcheck_review(
                    title=title,
                    url=url,
                    phone_model_id=model_key,
                    search_kw=search_kw,
                )

    print("\nâœ… Notebookcheck æŠ“å–ç»“æŸã€‚")


if __name__ == "__main__":
    # ç°åœ¨åªè·‘ Notebookcheckï¼Œä¸è·‘ GSM
    crawl_notebookcheck_only()
