import requests
import pandas as pd
import time
import random
from datetime import datetime
import os
import json

import config

# ä»£ç†è®¾ç½®
PROXIES = {
    "http": "http://127.0.0.1:7890",
    "https": "http://127.0.0.1:7890",
}

# è¿›åº¦æ–‡ä»¶
PROGRESS_FILENAME = "reddit_progress.json"

# æœ¬æ¬¡è¿è¡Œä½¿ç”¨çš„å…¨å±€ CSV æ–‡ä»¶åï¼ˆä¼šåœ¨ init_run é‡Œåˆå§‹åŒ–ï¼‰
CSV_FILENAME = None


def save_progress(progress: dict):
    """æŠŠè¿›åº¦å†™å…¥åˆ°æœ¬åœ° JSON æ–‡ä»¶"""
    try:
        with open(PROGRESS_FILENAME, "w", encoding="utf-8") as f:
            json.dump(progress, f, ensure_ascii=False, indent=2)
    except Exception as e:
        print(f"âš ï¸ ä¿å­˜è¿›åº¦å¤±è´¥: {e}")


def init_run():
    """
    åˆå§‹åŒ–æœ¬æ¬¡è¿è¡Œçš„ CSV æ–‡ä»¶åå’Œè¿›åº¦ï¼š
    - å¦‚æœå‘ç°æœ‰æœªå®Œæˆçš„è¿›åº¦æ–‡ä»¶ï¼Œä¸”å¯¹åº” CSV è¿˜åœ¨ï¼Œåˆ™ç»§ç»­ä¸Šæ¬¡çš„ä½ç½®ï¼›
    - å¦åˆ™æ–°å»ºä¸€ä¸ª CSV å’Œè¿›åº¦æ–‡ä»¶ã€‚
    """
    global CSV_FILENAME

    progress = {}
    if os.path.exists(PROGRESS_FILENAME):
        try:
            with open(PROGRESS_FILENAME, "r", encoding="utf-8") as f:
                progress = json.load(f)
        except Exception as e:
            print(f"âš ï¸ è¯»å–è¿›åº¦æ–‡ä»¶å¤±è´¥ï¼Œå°†é‡æ–°å¼€å§‹: {e}")
            progress = {}

    csv_from_progress = progress.get("csv_filename")
    models_state = progress.get("models", {}) or {}

    # åˆ¤æ–­æ˜¯å¦è¿˜æœ‰æ¨¡å‹æœªå®Œæˆï¼ˆåŒ…æ‹¬ progress ä¸­æ²¡æœ‰è®°å½•çš„å‹å·ï¼‰
    has_incomplete = False
    if models_state:
        for model_key in config.TARGET_MODELS.keys():
            state = models_state.get(model_key)
            if not state or not state.get("completed"):
                has_incomplete = True
                break

    if csv_from_progress and os.path.exists(csv_from_progress) and has_incomplete:
        # ç»­è·‘
        CSV_FILENAME = csv_from_progress
        print(f"ğŸ”„ æ£€æµ‹åˆ°æœªå®Œæˆçš„ä»»åŠ¡ï¼Œå°†ç»§ç»­å†™å…¥: {CSV_FILENAME}")
        progress.setdefault("models", models_state)
        return progress

    # å¦åˆ™å¼€å¯ä¸€ä¸ªæ–°çš„ä»»åŠ¡
    CSV_FILENAME = f"data_reddit_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv"
    progress = {
        "csv_filename": CSV_FILENAME,
        "models": {}
    }
    save_progress(progress)
    print(f"ğŸ†• å¼€å§‹æ–°çš„æŠ“å–ä»»åŠ¡ï¼Œæœ¬æ¬¡æ•°æ®å°†å†™å…¥: {CSV_FILENAME}")
    return progress


def append_row_to_csv(row: dict):
    """æŠŠä¸€æ¡è®°å½•è¿½åŠ å†™å…¥åˆ° CSVï¼Œæ–‡ä»¶ä¸å­˜åœ¨æ—¶å†™è¡¨å¤´ï¼ˆå®æ—¶å†™ç›˜ï¼‰"""
    if CSV_FILENAME is None:
        raise RuntimeError("CSV_FILENAME å°šæœªåˆå§‹åŒ–ï¼Œè¯·å…ˆè°ƒç”¨ init_run()")

    df = pd.DataFrame([row])
    file_exists = os.path.exists(CSV_FILENAME)
    df.to_csv(
        CSV_FILENAME,
        mode="a",                # è¿½åŠ å†™å…¥
        index=False,
        encoding="utf-8-sig",
        header=not file_exists,  # åªæœ‰ç¬¬ä¸€æ¬¡å†™å…¥æ‰å†™è¡¨å¤´
    )


def load_seen_ids_from_csv() -> set:
    """ä»å·²æœ‰ CSV ä¸­æŠŠå·²ç»æŠ“è¿‡çš„ source_id è¯»å‡ºæ¥ï¼Œé¿å…å¤šæ¬¡è¿è¡Œäº§ç”Ÿé‡å¤"""
    seen_ids = set()
    if CSV_FILENAME and os.path.exists(CSV_FILENAME):
        try:
            df_exist = pd.read_csv(CSV_FILENAME, usecols=["source_id"])
            seen_ids = set(df_exist["source_id"].dropna().astype(str).tolist())
            print(f"ğŸ” å·²ä»å†å² CSV åŠ è½½ {len(seen_ids)} æ¡ source_idï¼Œé¿å…é‡å¤ã€‚")
        except Exception as e:
            print(f"âš ï¸ è¯»å–å†å² CSV å¤±è´¥ï¼Œå¯èƒ½ä¼šäº§ç”Ÿå°‘é‡é‡å¤æ•°æ®: {e}")
    return seen_ids


def crawl_reddit_by_model():
    # åˆå§‹åŒ–æœ¬æ¬¡è¿è¡Œï¼ˆå†³å®šæ˜¯å¦ç»­è·‘ & CSV æ–‡ä»¶åï¼‰
    progress = init_run()
    models_state = progress.get("models", {})
    all_data = []

    # è¯»å–å†å² CSV ä¸­çš„ source_idï¼Œè·¨å¤šæ¬¡è¿è¡Œå»é‡
    seen_ids = load_seen_ids_from_csv()

    # æ¨¡æ‹Ÿæ›´çœŸå®çš„æµè§ˆå™¨å¤´
    headers = {
        "User-Agent": (
            "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
            "AppleWebKit/537.36 (KHTML, like Gecko) "
            "Chrome/123.0.0.0 Safari/537.36"
        ),
        "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,"
                  "image/avif,image/webp,*/*;q=0.8"
    }

    print(f"ğŸš€ å¼€å§‹æŠ“å– Redditï¼Œå…± {len(config.TARGET_MODELS)} ä¸ªå…·ä½“æœºå‹...")
    print(f"ğŸ“ æ•°æ®å°†å®æ—¶å†™å…¥: {CSV_FILENAME}")

    for model_key, keywords in config.TARGET_MODELS.items():
        if not keywords:
            continue

        # è¯»å‡ºè¯¥å‹å·çš„å†å²è¿›åº¦ï¼ˆå¦‚æœæœ‰ï¼‰
        state = models_state.get(model_key, {})
        if state.get("completed"):
            print(f"\n--- å‹å·: {model_key} å·²å®Œæˆï¼Œè·³è¿‡ ---")
            continue

        after_token = state.get("after_token")  # ä¸Šæ¬¡åœç•™çš„ after
        current_page = state.get("current_page", 0)
        search_term = keywords[0]

        if current_page > 0:
            print(f"\n--- ç»§ç»­å‹å·: {model_key} ({search_term}) "
                  f"ä»ç¬¬ {current_page + 1} é¡µå¼€å§‹ ---")
        else:
            print(f"\n--- æ­£åœ¨æœç´¢å‹å·: {model_key} ({search_term}) ---")

        # --- ç¿»é¡µé…ç½® ---
        max_pages = 5  # æ¯ä¸ªå‹å·æœ€å¤šæŠ“å‡ é¡µï¼ˆé˜²æ­¢æ­»å¾ªç¯ï¼‰

        while current_page < max_pages:
            try:
                # æ„é€ å‚æ•°ï¼šlimit=100 (æœ€å¤§å€¼), after=ç¿»é¡µæ ‡è®°
                params = {
                    "q": search_term,
                    "limit": 100,        # æ¯é¡µæœ€å¤š 100 æ¡
                    "sort": "new",
                    "type": "link",
                    "after": after_token  # å‘Šè¯‰ Reddit æˆ‘è¦çœ‹ä¸‹ä¸€é¡µ
                }

                resp = requests.get(
                    "https://www.reddit.com/search.json",
                    headers=headers,
                    params=params,
                    timeout=15,
                    proxies=PROXIES,
                )

                if resp.status_code != 200:
                    print(
                        f"âš ï¸ ç¬¬ {current_page + 1} é¡µè¯·æ±‚å¤±è´¥ "
                        f"({resp.status_code})ï¼Œè·³è¿‡è¯¥å‹å·å‰©ä½™é¡µ..."
                    )
                    break  # è¿™ä¸€é¡µæŒ‚äº†å°±åœä¸‹ï¼Œé˜²æ­¢æ­»å¾ªç¯è¯·æ±‚

                data = resp.json()
                children = data.get("data", {}).get("children", [])

                if not children:
                    print(f"   ç¬¬ {current_page + 1} é¡µæ— æ•°æ®ï¼Œåœæ­¢ç¿»é¡µã€‚")
                    # è¯¥å‹å·è§†ä¸ºå®Œæˆ
                    models_state[model_key] = {
                        "after_token": None,
                        "current_page": current_page,
                        "completed": True,
                    }
                    progress["models"] = models_state
                    save_progress(progress)
                    break

                print(f"   âœ… ç¬¬ {current_page + 1} é¡µæŠ“å–åˆ° {len(children)} æ¡ raw æ•°æ®")

                # å¤„ç†æ•°æ®
                for post in children:
                    p = post.get("data", {})
                    title = p.get("title", "") or ""
                    selftext = p.get("selftext", "") or ""
                    full_text = f"{title} {selftext}"

                    # åŒ¹é…å…³é”®è¯
                    if not any(kw.lower() in full_text.lower() for kw in keywords):
                        continue

                    # ç»„è£…å”¯ä¸€ ID
                    post_id = p.get("id")
                    if not post_id:
                        continue
                    source_id = f"reddit_{post_id}"

                    # å»é‡ï¼ˆè·¨å¤šæ¬¡è¿è¡Œï¼‰
                    if source_id in seen_ids:
                        continue
                    seen_ids.add(source_id)

                    # åŒ¹é…å“ç‰Œ
                    brand_name = "Other"
                    for b_code, b_name in config.BRANDS.items():
                        if b_code in model_key:
                            brand_name = b_name
                            break

                    # æ—¶é—´æˆ³è½¬æ—¶é—´
                    created_ts = p.get("created_utc")
                    if created_ts:
                        published_str = datetime.fromtimestamp(
                            created_ts
                        ).strftime("%Y-%m-%d %H:%M:%S")
                    else:
                        published_str = ""

                    row = {
                        "platform": "reddit",
                        "source_id": source_id,
                        "source_type": "post",
                        "url": f"https://www.reddit.com{p.get('permalink', '')}",
                        "brand_id": brand_name,
                        "phone_model_id": model_key,
                        "lang": "en",
                        "published_at": published_str,
                        "raw_text": f"{title}\n{selftext[:500]}",
                        "cleaned_text": title,
                    }

                    # å†…å­˜é‡Œå­˜ä¸€ä»½ï¼Œæ–¹ä¾¿ç»Ÿè®¡
                    all_data.append(row)
                    # ç«‹åˆ»å†™å…¥ CSVï¼ˆå®æ—¶å­˜ï¼‰
                    append_row_to_csv(row)

                # è·å–ä¸‹ä¸€é¡µçš„æ ‡è®°
                after_token = data.get("data", {}).get("after")
                current_page += 1

                # æ›´æ–°è¯¥å‹å·çš„è¿›åº¦ï¼ˆä¸­æ­¢è‡ªåŠ¨å­˜ï¼‰
                completed = not after_token or current_page >= max_pages
                models_state[model_key] = {
                    "after_token": after_token,
                    "current_page": current_page,
                    "completed": completed,
                }
                progress["models"] = models_state
                save_progress(progress)

                # å¦‚æœæ²¡æœ‰ after tokenï¼Œè¯´æ˜æ²¡æœ‰ä¸‹ä¸€é¡µäº†
                if not after_token:
                    print("   æ²¡æœ‰æ›´å¤šé¡µé¢äº†ã€‚")
                    break

                # ç¿»é¡µä¹‹é—´å¿…é¡»ä¼‘æ¯ï¼ŒReddit å®¹æ˜“ 429 Too Many Requests
                time.sleep(random.uniform(2, 4))

            except Exception as e:
                print(f"âŒ æŠ“å–å‡ºé”™: {e}")
                # å‡ºé”™æ—¶ä¹Ÿä¿å­˜å½“å‰çš„è¿›åº¦ï¼ˆå·²åœ¨ä¸Šä¸€æ¬¡æˆåŠŸé¡µä¿å­˜è¿‡ï¼‰
                break  # å‡ºé”™è·³å‡ºå½“å‰å‹å·å¾ªç¯

    # ç»“æŸç»Ÿè®¡
    if all_data:
        print(
            f"\nâœ… Reddit æŠ“å–å®Œæˆ/æœ¬æ¬¡è¿è¡Œç»“æŸï¼æœ¬æ¬¡æ–°å¢ {len(all_data)} æ¡æœ‰æ•ˆæ•°æ®ï¼Œ"
            f"å·²è¿½åŠ ä¿å­˜åˆ° {CSV_FILENAME}"
        )
    else:
        print("âš ï¸ æœ¬æ¬¡è¿è¡ŒæœªæŠ“å–åˆ°æ–°æ•°æ®ï¼ˆå¯èƒ½æ˜¯å…¨éƒ¨éƒ½å·²ç»åœ¨å†å² CSV ä¸­ï¼‰ã€‚")


if __name__ == "__main__":
    try:
        crawl_reddit_by_model()
    except KeyboardInterrupt:
        print("\nğŸ›‘ æ‰‹åŠ¨ä¸­æ­¢æŠ“å–ï¼Œè¿›åº¦å’Œå·²æŠ“å–æ•°æ®å·²ç»ä¿å­˜ï¼ˆå¯ä¸‹æ¬¡ç»§ç»­è¿è¡Œç»­çˆ¬ï¼‰ã€‚")
