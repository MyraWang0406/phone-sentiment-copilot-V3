import os
import time
import random
import re
from datetime import datetime

import requests
import pandas as pd

import config

# =========================
# åŸºæœ¬é…ç½®
# =========================

# âœ… æ–°æ–‡ä»¶åï¼Œé¿å…è·Ÿä¹‹å‰è·‘çš„æ··åœ¨ä¸€èµ·
CSV_FILENAME = "data_bilibili_v2.csv"

MAX_VIDEOS_PER_MODEL = 30      # æ¯ä¸ªæœºå‹æœ€å¤šæŠ“å¤šå°‘è§†é¢‘
MAX_COMMENTS_PER_VIDEO = 30    # æ¯ä¸ªè§†é¢‘æœ€å¤šæŠ“å¤šå°‘æ¡è¯„è®ºï¼ˆå‰ 30 æ¡çƒ­é—¨ï¼‰

HEADERS = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
                  "AppleWebKit/537.36 (KHTML, like Gecko) "
                  "Chrome/124.0 Safari/537.36",
    "Referer": "https://www.bilibili.com/",
}

# ===== ç™»å½• Cookieï¼ˆä» DevTools é‡Œå¤åˆ¶è¿‡æ¥çš„é‚£ä¸€æ•´æ®µ valueï¼‰=====
# âš ï¸ æ³¨æ„ï¼šè¿™é‡ŒåŒ…å«ä½ çš„ç™»é™†èº«ä»½ä¿¡æ¯ï¼Œè¯·ä¸è¦æŠŠè¿™ä¸ªæ–‡ä»¶å¯¹å¤–åˆ†äº«
BILIBILI_COOKIE = (
    "buvid3=BB9B4F7D-4802-694D-0B40-E1566BBFCFBC76161infoc; "
    "b_nut=1764996576; "
    "b_lsid=26B6565C_19AF1FE7B85; "
    "_uuid=C2994A510-5B54-EBB10-110E7-817FF55CF9EE78194infoc; "
    "buvid_fp=53a1d05875fd5fffd24ee2833f7add7d; "
    "buvid4=24CDD821-3713-9F04-4E63-07A76666E51C78607-025120612-BAMKQ6YEN+Yk2N6xTwm3yw%3D%3D; "
    "CURRENT_QUALITY=0; "
    "bili_ticket=eyJhbGciOiJIUzI1NiIsImtpZCI6InMwMyIsInR5cCI6IkpXVCJ9.eyJleHAiOjE3NjUyNTU3ODIsImlhdCI6MTc2NDk5NjUyMiwicGx0IjotMX0.9dnIDJIEPN6kLiGCP-AjNppZ-98APwvd4Sku4g2_q4Q; "
    "bili_ticket_expires=1765255722; "
    "rpdid=|(J~R~JJ)m)R0J'u~YRmmYl)J; "
    "CURRENT_FNVAL=2000; "
    "SESSDATA=33b95b9b%2C1780550431%2C26c95%2Ac2CjD5WfQg3mcRJQFLjwlJ8xEwmMk5lu44DtxH6HkKtjIr1w9icmxn_Vh6cfVtIY1RBgkSVkRDNzkxaC1IajFCM2FWcl9KZGc1bDlMTEhXSTVrLUgzbnQxMUozbmpHYnl6cFc3Rjh4ci10cGZMNXdUUFVQaDJYQlpGSEVub0k2VEowS1pMaE5VaC1nIIEC; "
    "bili_jct=7d690342832787b86a8a378b8c8620ac; "
    "DedeUserID=38638790; "
    "DedeUserID__ckMd5=ecf9c0bc13c38c69; "
    "sid=gxpnozgm; "
    "bp_t_offset_38638790=1143148684481921024; "
    "theme-tip-show=SHOWED"
)

if BILIBILI_COOKIE:
    HEADERS["Cookie"] = BILIBILI_COOKIE

# æœç´¢ / è¯¦æƒ… / è¯„è®º API
SEARCH_API = "https://api.bilibili.com/x/web-interface/search/type"
VIEW_API = "https://api.bilibili.com/x/web-interface/view"
REPLY_API = "https://api.bilibili.com/x/v2/reply/main"

# BV å·æ­£åˆ™
BVID_RE = re.compile(r"BV[0-9A-Za-z]{10}")


# =========================
# å·¥å…·å‡½æ•°ï¼šCSV è¿½åŠ  & è¯·æ±‚
# =========================

def append_row_to_csv(row: dict):
    """æŠŠä¸€æ¡è®°å½•å®æ—¶è¿½åŠ å†™å…¥ CSVï¼Œæ–‡ä»¶ä¸å­˜åœ¨æ—¶è‡ªåŠ¨å†™è¡¨å¤´"""
    df = pd.DataFrame([row])
    file_exists = os.path.exists(CSV_FILENAME)
    df.to_csv(
        CSV_FILENAME,
        mode="a",
        index=False,
        encoding="utf-8-sig",
        header=not file_exists,
    )


def get_json(url: str, params: dict = None, sleep_range=(0.3, 0.8)):
    """ç®€æ˜“ GET JSONï¼ˆç”¨äº B ç«™ APIï¼‰"""
    for i in range(3):
        try:
            resp = requests.get(url, headers=HEADERS, params=params, timeout=15)
            if resp.status_code == 200:
                time.sleep(random.uniform(*sleep_range))
                return resp.json()
            else:
                print(f"   âš ï¸ API {url} è¿”å› {resp.status_code}")
        except Exception as e:
            print(f"   âš ï¸ API {url} å‡ºé”™: {e}")
        time.sleep(1.0 + i)
    return None


# =========================
# æ–­ç‚¹ç»­è·‘ï¼šä» CSV é‡Œæ¢å¤è¿›åº¦
# =========================

SEEN_VIDEO_URLS = set()       # å·²ç»æŠ“è¿‡çš„è§†é¢‘ URL
VIDEOS_DONE_PER_MODEL = {}    # æ¯ä¸ªæœºå‹å·²ç»æŠ“è¿‡å¤šå°‘æ¡è§†é¢‘


def load_existing_progress():
    """ä»å†å² CSV æ¢å¤å»é‡ & æ¯æœºå‹å·²æŠ“è§†é¢‘æ•°"""
    global SEEN_VIDEO_URLS, VIDEOS_DONE_PER_MODEL

    if not os.path.exists(CSV_FILENAME):
        print(f"ğŸ“ æœªå‘ç°å†å²æ–‡ä»¶ï¼Œå°†ä»å¤´å¼€å§‹æŠ“å–å¹¶å†™å…¥ {CSV_FILENAME}")
        return

    try:
        df = pd.read_csv(CSV_FILENAME)
    except Exception as e:
        print(f"âš ï¸ è¯»å–å†å² CSV å¤±è´¥ï¼Œä¸å¯ç”¨æ–­ç‚¹ç»­çˆ¬é€»è¾‘: {e}")
        return

    if df.empty:
        print(f"ğŸ“‚ å‘ç°å†å²æ–‡ä»¶ {CSV_FILENAME}ï¼Œä½†ä¸ºç©ºï¼Œå°†ä»å¤´å¼€å§‹æŠ“ã€‚")
        return

    print(f"ğŸ“‚ æ£€æµ‹åˆ°å·²æœ‰å†å²æ–‡ä»¶ {CSV_FILENAME}ï¼Œæ€»è®°å½• {len(df)} è¡Œã€‚")

    if "url" in df.columns and "data_type" in df.columns:
        video_df = df[df["data_type"] == "video"].copy()
        SEEN_VIDEO_URLS = set(video_df["url"].dropna().unique())

        if {"phone_model_id", "url"}.issubset(video_df.columns):
            VIDEOS_DONE_PER_MODEL = (
                video_df.groupby("phone_model_id")["url"]
                .nunique()
                .to_dict()
            )

        print(f"   ğŸ” å·²æœ‰è§†é¢‘ {len(SEEN_VIDEO_URLS)} æ¡ï¼Œ"
              f"{len(VIDEOS_DONE_PER_MODEL)} ä¸ªæœºå‹æœ‰å†å²è®°å½•ã€‚")


# =========================
# 1. æœç´¢åˆ—è¡¨é¡µï¼ˆæœç´¢ APIï¼‰
# =========================

def parse_play_count(text: str) -> int:
    """æŠŠ B ç«™çš„æ’­æ”¾é‡å­—ç¬¦ä¸² (ä¾‹ï¼š'29.5ä¸‡') è½¬æˆæ•´æ•°"""
    if text is None:
        return 0
    if isinstance(text, (int, float)):
        return int(text)
    text = str(text).strip()
    try:
        if text.endswith("ä¸‡"):
            return int(float(text[:-1]) * 10000)
        if text.endswith("äº¿"):
            return int(float(text[:-1]) * 100000000)
        return int("".join(ch for ch in text if ch.isdigit()))
    except Exception:
        return 0


TAG_RE = re.compile(r"<.*?>")


def strip_html_tags(s: str) -> str:
    if not s:
        return ""
    return TAG_RE.sub("", s)


def search_bilibili_videos(search_kw: str, page_no: int):
    """
    ç”¨ B ç«™æœç´¢ API æŒ‰å…³é”®è¯æŸ¥è§†é¢‘ï¼Œè¿”å›æœ¬é¡µçš„è§†é¢‘ä¿¡æ¯åˆ—è¡¨ï¼š
    [ {title, url, bvid, up_name, play_str, pubtime_str}, ... ]
    """
    params = {
        "search_type": "video",
        "keyword": search_kw,
        "page": page_no,
    }
    print(f"\n   ğŸ‘‰ ç¬¬ {page_no} é¡µæœç´¢(API)ï¼š{SEARCH_API}  keyword={search_kw}")
    data = get_json(SEARCH_API, params=params)
    if not data:
        print("   âš ï¸ æœç´¢ API æ— å“åº”ã€‚")
        return []
    if data.get("code") != 0:
        print(f"   âš ï¸ æœç´¢ API è¿”å›é”™è¯¯ code={data.get('code')} msg={data.get('message')}")
        return []

    d = data.get("data") or {}
    results_raw = d.get("result") or []
    print(f"   ğŸ” æœç´¢ API è¿”å›ç»“æœæ•°ï¼š{len(results_raw)}")

    results = []
    for item in results_raw:
        title_html = item.get("title") or ""
        title = strip_html_tags(title_html).strip()

        url = item.get("arcurl") or ""
        bvid = item.get("bvid") or ""

        if not url and bvid:
            url = f"https://www.bilibili.com/video/{bvid}"

        if not url or not title:
            continue

        up_name = (item.get("author") or "").strip()

        play_raw = item.get("play")
        play_str = str(play_raw) if play_raw is not None else ""

        pub_ts = item.get("pubtime") or item.get("pubdate")
        if isinstance(pub_ts, (int, float)) and pub_ts > 0:
            try:
                pubtime_str = datetime.fromtimestamp(pub_ts).strftime("%Y-%m-%d %H:%M:%S")
            except Exception:
                pubtime_str = ""
        else:
            pubtime_str = ""

        results.append(
            {
                "title": title,
                "url": url,
                "bvid": bvid,
                "up_name": up_name,
                "play_str": play_str,
                "pubtime_str": pubtime_str,
            }
        )

    return results


# =========================
# 2. B ç«™è¯„è®ºæ¥å£ï¼šview + reply/main
# =========================

def extract_bvid(url: str):
    """ä»è§†é¢‘ URL ä¸­æŠ½å‡º BV å·ï¼ˆå…œåº•ï¼‰"""
    if not url:
        return None
    m = BVID_RE.search(url)
    if m:
        return m.group(0)
    return None


def fetch_aid_from_bvid(bvid: str):
    """é€šè¿‡ bvid è°ƒç”¨ view æ¥å£æ‹¿ aid"""
    params = {"bvid": bvid}
    data = get_json(VIEW_API, params=params)
    if not data or data.get("code") != 0:
        return None
    try:
        return int(data["data"]["aid"])
    except Exception:
        return None


def fetch_comments_by_api(bvid: str, max_comments: int = 30):
    """
    è°ƒç”¨ B ç«™è¯„è®º APIï¼ŒæŒ‰â€œçƒ­åº¦â€æ‹¿å‰ max_comments æ¡è¯„è®ºã€‚
    """
    aid = fetch_aid_from_bvid(bvid)
    if not aid:
        print(f"      âš ï¸ æ— æ³•è·å– aidï¼Œbvid={bvid}")
        return []

    comments = []
    page_no = 1
    page_size = 30  # API å•é¡µæœ€å¤š 30

    while True:
        if max_comments is not None and len(comments) >= max_comments:
            break

        params = {
            "type": 1,       # 1 = è§†é¢‘
            "oid": aid,
            "mode": 3,       # 3 = æŒ‰çƒ­åº¦æ’åº
            "ps": page_size,
            "pn": page_no,
        }
        print(f"      ğŸ’¬ æ‹‰å–è¯„è®ºç¬¬ {page_no} é¡µï¼Œå½“å‰å·²æœ‰ {len(comments)} æ¡...")
        data = get_json(REPLY_API, params=params, sleep_range=(0.3, 0.7))
        if not data or data.get("code") != 0:
            print(f"      âš ï¸ è¯„è®º API è¿”å›å¼‚å¸¸ï¼Œcode={data.get('code') if data else 'N/A'}")
            break

        d = data.get("data") or {}
        replies = d.get("replies") or []
        if not replies:
            break

        for rep in replies:
            if max_comments is not None and len(comments) >= max_comments:
                break

            content = ((rep.get("content") or {}).get("message") or "").strip()
            if not content:
                continue

            member = rep.get("member") or {}
            uname = (member.get("uname") or "").strip()

            like = int(rep.get("like") or 0)
            ctime = int(rep.get("ctime") or 0)
            try:
                comment_time = datetime.fromtimestamp(ctime).isoformat(timespec="seconds")
            except Exception:
                comment_time = ""

            comments.append(
                {
                    "author": uname,
                    "comment_time": comment_time,
                    "comment_like": like,
                    "content": content,
                }
            )

        # æœ¬é¡µæ•°é‡ä¸è¶³ page_sizeï¼Œè¯´æ˜å·²ç»åˆ°åº•äº†
        if len(replies) < page_size:
            break

        page_no += 1

    return comments


# =========================
# 3. ä¸»æµç¨‹ï¼šæŒ‰æœºå‹æŠ“ B ç«™
# =========================

def crawl_bilibili_by_model():
    print("ğŸš€ å¼€å§‹æŠ“å– B ç«™ï¼ˆæ¯æœºå‹æœ€å¤š 30 ä¸ªè§†é¢‘ + æ¯è§†é¢‘å‰ 30 æ¡è¯„è®ºï¼‰")
    print(f"ğŸ“ æ•°æ®ä¼šå®æ—¶å†™å…¥: {CSV_FILENAME}")

    load_existing_progress()

    total_videos = 0
    total_comments = 0
    total_rows = 0

    # ç»Ÿè®¡å·²æœ‰ï¼ˆæ–¹ä¾¿æ‰“å°æ€»é‡ï¼‰
    if os.path.exists(CSV_FILENAME):
        try:
            df_old = pd.read_csv(CSV_FILENAME)
            total_rows = len(df_old)
            if "data_type" in df_old.columns:
                total_videos = (
                    df_old[df_old["data_type"] == "video"]["url"]
                    .dropna()
                    .nunique()
                )
                total_comments = (df_old["data_type"] == "comment").sum()
        except Exception:
            pass

    # é€ä¸ªæœºå‹
    for idx, (model_key, keywords) in enumerate(config.TARGET_MODELS.items(), start=1):
        if not keywords:
            continue

        search_kw = keywords[0]
        videos_this_model = int(VIDEOS_DONE_PER_MODEL.get(model_key, 0))

        print(
            f"\n================ æœºå‹ {idx}: {model_key} ({search_kw}) ================"
        )
        print(f"   ï¼ˆå†å²å·²æŠ“ {videos_this_model} ä¸ªè§†é¢‘ï¼Œæœ¬è½®ä¸Šé™ {MAX_VIDEOS_PER_MODEL} ä¸ªï¼‰")

        if videos_this_model >= MAX_VIDEOS_PER_MODEL:
            print(f"   â›” è¯¥æœºå‹å†å²è§†é¢‘æ•°å·²è¾¾ä¸Šé™ {MAX_VIDEOS_PER_MODEL}ï¼Œè·³è¿‡ã€‚")
            continue

        # åªè¦æ²¡å¤Ÿ 30 ä¸ªè§†é¢‘ï¼Œå°±ä¸€é¡µä¸€é¡µå¾€åç¿»ï¼Œç›´åˆ°æœç´¢ç»“æœç©º
        page_no = 1
        while videos_this_model < MAX_VIDEOS_PER_MODEL:
            videos = search_bilibili_videos(search_kw, page_no)
            if not videos:
                print("   â›” æœ¬é¡µæ— æœç´¢ç»“æœï¼Œç»“æŸè¯¥æœºå‹ã€‚")
                break

            print(f"   âœ… ç¬¬ {page_no} é¡µå…± {len(videos)} æ¡æœç´¢ç»“æœã€‚")

            for vid in videos:
                if videos_this_model >= MAX_VIDEOS_PER_MODEL:
                    break

                url = vid["url"]
                title = vid["title"]

                # æ–­ç‚¹ç»­çˆ¬ï¼šå·²ç»æŠ“è¿‡çš„è§†é¢‘ç›´æ¥è·³è¿‡
                if url in SEEN_VIDEO_URLS:
                    continue
                SEEN_VIDEO_URLS.add(url)

                play_count = parse_play_count(vid["play_str"])

                source_id = f"bilibili_{int(time.time() * 1000)}_{random.randint(1000, 9999)}"

                base_row = {
                    "platform": "bilibili",
                    "source_id": source_id,
                    "url": url,
                    "phone_model_id": model_key,
                    "data_type": "video",
                    "raw_text": f"[UP: {vid['up_name'] or 'æœªçŸ¥'}][æ’­æ”¾: {vid['play_str'] or 'æ— '}]"
                                f"[æ—¶é—´: {vid['pubtime_str'] or 'æ— '}] {title}",
                    "cleaned_text": title,
                    "search_kw": search_kw,
                    "up_name": vid["up_name"],
                    "play_str": vid["play_str"],
                    "play_count": play_count,
                    "pubtime_str": vid["pubtime_str"],
                    "created_at": datetime.now().isoformat(timespec="seconds"),
                }

                print(
                    f"   ğŸ¬ è§†é¢‘: {title[:50]}... | UP: {vid['up_name'] or 'æœªçŸ¥'} | æ’­æ”¾: {vid['play_str'] or 'æ— '}"
                )

                # ---- è¯„è®ºï¼šç”¨ API æŠ“å‰ 30 æ¡ ----
                bvid = vid.get("bvid") or extract_bvid(url)
                comment_rows = []
                if bvid:
                    comments = fetch_comments_by_api(bvid, MAX_COMMENTS_PER_VIDEO)
                    for c_idx, c in enumerate(comments):
                        row = {
                            **base_row,
                            "source_id": f"{source_id}_c{c_idx}",
                            "data_type": "comment",
                            "raw_text": f"[è¯„è®º] {c['content']}",
                            "cleaned_text": c["content"],
                            "comment_author": c["author"],
                            "comment_time": c["comment_time"],
                            "comment_like": c["comment_like"],
                        }
                        comment_rows.append(row)
                else:
                    print(f"      âš ï¸ æ— æ³•ä» URL ä¸­è§£æå‡º bvid: {url}")

                # å…ˆå†™è§†é¢‘è¡Œ
                append_row_to_csv(base_row)
                total_videos += 1
                total_rows += 1
                videos_this_model += 1

                # å†å†™è¯„è®ºè¡Œ
                for cr in comment_rows:
                    append_row_to_csv(cr)
                    total_comments += 1
                    total_rows += 1

                print(
                    f"      âœ… å·²å†™å…¥è§†é¢‘ 1 æ¡ï¼Œæœ¬è§†é¢‘è¯„è®º {len(comment_rows)} æ¡ï¼Œ"
                    f"æœ¬æœºå‹ç´¯è®¡è§†é¢‘ {videos_this_model} æ¡ï¼Œæ‰€æœ‰æœºå‹æ€»è¯„è®º {total_comments} æ¡ã€‚"
                )

                # è§†é¢‘ä¹‹é—´ä¼‘æ¯ä¸€ä¸‹ï¼Œåˆ«æŠŠæ¥å£æ‰“çˆ†
                time.sleep(random.uniform(0.5, 1.0))

            page_no += 1
            # ç¿»é¡µä¹‹é—´ä¹Ÿæ­‡ä¸€ä¸‹
            time.sleep(random.uniform(0.8, 1.5))

        print(
            f"   ğŸ“Š æœºå‹ {model_key} å®Œæˆç´¯è®¡ï¼šè§†é¢‘ {videos_this_model} æ¡ï¼Œ"
            f"æ‰€æœ‰æœºå‹æ€»è¯„è®º {total_comments} æ¡ã€‚"
        )

    if total_rows > 0:
        print(
            f"\nâœ… æŠ“å–ç»“æŸ / å½“å‰çŠ¶æ€ï¼šå…±è§†é¢‘ {total_videos} æ¡ï¼Œè¯„è®º {total_comments} æ¡ï¼Œ"
            f"æ€»è®°å½•æ•° {total_rows}ï¼Œå·²ä¿å­˜ä¸º {CSV_FILENAME}"
        )
    else:
        print("\nâš ï¸ æ²¡æœ‰æŠ“åˆ°ä»»ä½•æ•°æ®ï¼Œè¯·æ£€æŸ¥ç½‘ç»œæˆ–é€‰æ‹©å™¨ã€‚")


if __name__ == "__main__":
    crawl_bilibili_by_model()
